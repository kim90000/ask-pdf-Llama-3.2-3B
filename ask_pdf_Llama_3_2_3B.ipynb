{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiQx1tHvKfxe"
      },
      "outputs": [],
      "source": [
        "streamlit\n",
        "groq\n",
        "pypdf\n",
        "faiss-cpu\n",
        "sentence-transformers\n",
        "langchain\n",
        "langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Vikneshwara-kumar/RAG_Based_Talk_to_PDF"
      ],
      "metadata": {
        "id": "odxgNspFYzgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r a.txt"
      ],
      "metadata": {
        "id": "T5Ectu45L1Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fVMZi62BL5wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyoyPOJlM_FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5OV5nQoZM_B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "uPqd1RN_ZSRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict\n",
        "from pypdf import PdfReader\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class PDFQASystem:\n",
        "    def __init__(self, model_id: str):\n",
        "        \"\"\"Initialize the PDF QA System with necessary components.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "        )\n",
        "        self.vector_store = None\n",
        "\n",
        "    def process_pdf(self, pdf_file) -> int:\n",
        "        \"\"\"Process a PDF file and create embeddings.\"\"\"\n",
        "        pdf_text = self._extract_text_from_pdf(pdf_file)\n",
        "        chunks = self.text_splitter.split_text(pdf_text)\n",
        "        self.vector_store = FAISS.from_texts(chunks, self.embeddings)\n",
        "        return len(chunks)\n",
        "\n",
        "    def _extract_text_from_pdf(self, pdf_file) -> str:\n",
        "        \"\"\"Extract text content from a PDF file.\"\"\"\n",
        "        reader = PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    def generate_response(self, user_query: str, k: int = 4) -> str:\n",
        "        \"\"\"Generate a response using relevant context and the LLM.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return \"Please upload a PDF document first.\"\n",
        "\n",
        "        relevant_docs = self.vector_store.similarity_search(user_query, k=k)\n",
        "        context = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "        prompt = f\"\"\"You are a helpful assistant. Using the following context, please answer the user's question as detailed as possible from the provided context, make sure to provide all the details.\n",
        "        If the answer cannot be found in the context, say so.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {user_query}\n",
        "\n",
        "        Please structure your response as follows:\n",
        "        - **Key Points**: List each main point in bullet points.\n",
        "        - **Summary**: Provide a brief paragraph summarizing the main findings.\n",
        "\n",
        "        Answer:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True).to(self.model.device)\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=10)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "# Example usage in a Colab environment\n",
        "def main():\n",
        "    model_id = \"meta-llama/Llama-3.2-3B\"  # Updated model ID\n",
        "    qa_system = PDFQASystem(model_id)\n",
        "\n",
        "    pdf_path = \"/content/temp.pdf\"  # Replace with your PDF file path\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(\"PDF file not found!\")\n",
        "        return\n",
        "\n",
        "    print(\"Processing PDF...\")\n",
        "    num_chunks = qa_system.process_pdf(pdf_path)\n",
        "    print(f\"PDF processed into {num_chunks} chunks.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"Enter your question (or type 'exit' to quit): \")\n",
        "        if user_query.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        print(\"Generating response...\")\n",
        "        response = qa_system.generate_response(user_query)\n",
        "        print(\"\\nResponse:\\n\")\n",
        "        print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "dmjK5N4EM--Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict\n",
        "from pypdf import PdfReader\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class PDFQASystem:\n",
        "    def __init__(self, model_id: str):\n",
        "        \"\"\"Initialize the PDF QA System with necessary components.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "        )\n",
        "        self.vector_store = None\n",
        "\n",
        "    def process_pdf(self, pdf_file) -> int:\n",
        "        \"\"\"Process a PDF file and create embeddings.\"\"\"\n",
        "        pdf_text = self._extract_text_from_pdf(pdf_file)\n",
        "        chunks = self.text_splitter.split_text(pdf_text)\n",
        "        self.vector_store = FAISS.from_texts(chunks, self.embeddings)\n",
        "        return len(chunks)\n",
        "\n",
        "    def _extract_text_from_pdf(self, pdf_file) -> str:\n",
        "        \"\"\"Extract text content from a PDF file.\"\"\"\n",
        "        reader = PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    def generate_response(self, user_query: str, k: int = 4) -> str:\n",
        "        \"\"\"Generate a response using relevant context and the LLM.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return \"Please upload a PDF document first.\"\n",
        "\n",
        "        relevant_docs = self.vector_store.similarity_search(user_query, k=k)\n",
        "        context = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "        prompt = f\"\"\"You are a helpful assistant. Using the following context, please answer the user's question as detailed as possible from the provided context, make sure to provide all the details.\n",
        "        If the answer cannot be found in the context, say so.\n",
        "\n",
        "        Context:\n",
        "\n",
        "\n",
        "        Question: {user_query}\n",
        "\n",
        "        Please structure your response as follows:\n",
        "        - **Key Points**: List each main point in bullet points.\n",
        "        - **Summary**: Provide a brief paragraph summarizing the main findings.\n",
        "\n",
        "        Answer:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True).to(self.model.device)\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=50)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "# Example usage in a Colab environment\n",
        "def main():\n",
        "    model_id = \"meta-llama/Llama-3.2-3B\"  # Updated model ID\n",
        "    qa_system = PDFQASystem(model_id)\n",
        "\n",
        "    pdf_path = \"/content/temp.pdf\"  # Replace with your PDF file path\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(\"PDF file not found!\")\n",
        "        return\n",
        "\n",
        "    print(\"Processing PDF...\")\n",
        "    num_chunks = qa_system.process_pdf(pdf_path)\n",
        "    print(f\"PDF processed into {num_chunks} chunks.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"Enter your question (or type 'exit' to quit): \")\n",
        "        if user_query.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        print(\"Generating response...\")\n",
        "        response = qa_system.generate_response(user_query)\n",
        "        print(\"\\nResponse:\\n\")\n",
        "        print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "l5NP9OgcZ_Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "What is the book about?"
      ],
      "metadata": {
        "id": "_TT_lc3hOqJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "\n"
      ],
      "metadata": {
        "id": "6TZx9XdVNM3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict\n",
        "from pypdf import PdfReader\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "class PDFQASystem:\n",
        "    def __init__(self, model_id: str, device_map: str = \"auto\"):\n",
        "        \"\"\"Initialize the PDF QA System with necessary components.\"\"\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=device_map\n",
        "        )\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len,\n",
        "        )\n",
        "        self.vector_store = None\n",
        "\n",
        "    def process_pdf(self, pdf_file) -> int:\n",
        "        \"\"\"Process a PDF file and create embeddings.\"\"\"\n",
        "        pdf_text = self._extract_text_from_pdf(pdf_file)\n",
        "        chunks = self.text_splitter.split_text(pdf_text)\n",
        "        self.vector_store = FAISS.from_texts(chunks, self.embeddings)\n",
        "        return len(chunks)\n",
        "\n",
        "    def _extract_text_from_pdf(self, pdf_file) -> str:\n",
        "        \"\"\"Extract text content from a PDF file.\"\"\"\n",
        "        reader = PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    def generate_response(self, user_query: str, k: int = 4) -> str:\n",
        "        \"\"\"Generate a response using relevant context and the LLM.\"\"\"\n",
        "        if not self.vector_store:\n",
        "            return \"Please upload a PDF document first.\"\n",
        "\n",
        "        relevant_docs = self.vector_store.similarity_search(user_query, k=k)\n",
        "        context = \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "        prompt = f\"\"\"You are a helpful assistant. Using the following context, please answer the user's question as detailed as possible from the provided context, make sure to provide all the details.\n",
        "        If the answer cannot be found in the context, say so.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {user_query}\n",
        "\n",
        "        Please structure your response as follows:\n",
        "        - **Key Points**: List each main point in bullet points.\n",
        "        - **Summary**: Provide a brief paragraph summarizing the main findings.\n",
        "\n",
        "        Answer:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True).to(self.model.device)\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=300)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "\n",
        "# Example usage in a Colab environment\n",
        "def main():\n",
        "    model_id = \"meta-llama/Llama-3.2-3B\"  # Updated model ID\n",
        "    device_map = \"auto\"  # Specify the device map here\n",
        "    qa_system = PDFQASystem(model_id, device_map=device_map)\n",
        "\n",
        "    pdf_path = \"sample.pdf\"  # Replace with your PDF file path\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(\"PDF file not found!\")\n",
        "        return\n",
        "\n",
        "    print(\"Processing PDF...\")\n",
        "    num_chunks = qa_system.process_pdf(pdf_path)\n",
        "    print(f\"PDF processed into {num_chunks} chunks.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"Enter your question (or type 'exit' to quit): \")\n",
        "        if user_query.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        print(\"Generating response...\")\n",
        "        response = qa_system.generate_response(user_query)\n",
        "        print(\"\\nResponse:\\n\")\n",
        "        print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "BZkGBa4nNRtp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}